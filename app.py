import streamlit as st
from PIL import Image, ImageOps
import numpy as np
import torch
import torch.nn.functional as F
import os
import sys

import align

# =========================================================
# C·∫§U H√åNH
# =========================================================
MODEL_CHECKPOINT_PATH = "weights/adaface.ckpt" 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Import net.py
try:
    import net
except ImportError:
    st.error("‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y file `net.py`. H√£y ƒë·∫∑t c√πng th∆∞ m·ª•c.")
    st.stop()

st.set_page_config(page_title="AdaFace Demo", layout="centered")

# ------------------------
# 1. Load Model
# ------------------------
@st.cache_resource
def load_system_model():
    if not os.path.exists(MODEL_CHECKPOINT_PATH):
        st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: `{MODEL_CHECKPOINT_PATH}`")
        return None

    try:
        st.info(f"Loading model: `{MODEL_CHECKPOINT_PATH}` on `{DEVICE}`...")
        
        # Build Model Architecture
        model = net.build_model()
        
        # Load Weights
        checkpoint = torch.load(MODEL_CHECKPOINT_PATH, map_location=DEVICE)
        state_dict = checkpoint['state_dict'] if isinstance(checkpoint, dict) and 'state_dict' in checkpoint else checkpoint
        
        # Fix DataParallel keys
        new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
        
        model.load_state_dict(new_state_dict, strict=False)
        model.to(DEVICE)
        model.eval()
        return model
    except Exception as e:
        st.error(f"L·ªói load model: {e}")
        return None

# ------------------------
# 2. X·ª¨ L√ù ·∫¢NH THEO CODE C·ª¶A B·∫†N
# ------------------------

def extract_style_adaface(model, pil_img):
    """
    H√†m n√†y m√¥ ph·ªèng l·∫°i logic:
    np_img -> BGR convert -> Normalize th·ªß c√¥ng -> Tensor -> Model -> Normalize Feature
    """
    try:
        # 1. ƒê·∫£m b·∫£o Input l√† PIL RGB
        if pil_img.mode != "RGB":
            pil_img = pil_img.convert("RGB")
        
        # 2. Convert sang Numpy
        # L∆∞u √Ω: PIL m·∫∑c ƒë·ªãnh l√† RGB.
        np_img = np.array(pil_img) 
        
        # 3. Preprocessing (Logic c≈© c·ªßa b·∫°n)
        # ::-1 ƒë·ªÉ ƒë·∫£o chi·ªÅu k√™nh m√†u t·ª´ RGB sang BGR (quan tr·ªçng v·ªõi model InsightFace/AdaFace)
        bgr_img = ((np_img[:, :, ::-1] / 255.) - 0.5) / 0.5
        
        # 4. T·∫°o Tensor: (H, W, C) -> (C, H, W)
        tensor = torch.tensor(
            bgr_img.transpose(2, 0, 1)
        ).float().unsqueeze(0).to(DEVICE)

        # 5. Forward Pass
        with torch.no_grad():
            # Model AdaFace th∆∞·ªùng tr·∫£ v·ªÅ (feature, norm) ho·∫∑c ch·ªâ feature
            out = model(tensor)
            
            if isinstance(out, (tuple, list)):
                feature = out[0]
            else:
                feature = out
            
            # L·∫•y Norm g·ªëc ƒë·ªÉ check ch·∫•t l∆∞·ª£ng ·∫£nh (Optional)
            norm_val = torch.norm(feature, p=2, dim=1).item()

            # 6. Normalize Feature (Quan tr·ªçng)
            feature = F.normalize(feature, dim=1)
            
            # 7. Convert sang Numpy
            return feature.cpu().numpy()[0], norm_val

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω ·∫£nh: {e}")
        return None, 0.0

def compute_cosine(a, b):
    # D√πng Numpy dot product cho an to√†n
    return float(np.dot(a, b))

# ------------------------
# 3. Giao di·ªán Streamlit
# ------------------------
st.title("üîç AdaFace Verification")
model = load_system_model()

if not model:
    st.stop()

with st.sidebar:
    use_mtcnn = st.checkbox("D√πng MTCNN Crop", value=True)
    threshold = st.slider("Ng∆∞·ª°ng (Threshold)", 0.0, 1.0, 0.30, 0.01)

col1, col2 = st.columns(2)
f1 = col1.file_uploader("·∫¢nh 1", type=["jpg", "png", "jpeg"])
f2 = col2.file_uploader("·∫¢nh 2", type=["jpg", "png", "jpeg"])

if f1 and f2:
    st.write("---")
    c1, c2 = st.columns(2)
    
    # --- X·ª≠ l√Ω ·∫¢nh 1 ---
    img1 = Image.open(f1).convert("RGB")
    # B∆∞·ªõc Alignment (C·∫Øt m·∫∑t)
    if use_mtcnn:
        align1 = align.get_aligned_face(image_path=None, rgb_pil_image=img1)
        final_img1 = align1 if align1 else ImageOps.fit(img1, (112,112))
    else:
        final_img1 = ImageOps.fit(img1, (112,112)) # Center crop c∆° b·∫£n
        
    c1.image(final_img1, caption="Input 1 (Aligned)", width=150)
    # G·ªçi h√†m x·ª≠ l√Ω m·ªõi
    emb1, n1 = extract_style_adaface(model, final_img1)

    # --- X·ª≠ l√Ω ·∫¢nh 2 ---
    img2 = Image.open(f2).convert("RGB")
    if use_mtcnn:
        align2 = align.get_aligned_face(image_path=None, rgb_pil_image=img2)
        final_img2 = align2 if align2 else ImageOps.fit(img2, (112,112))
    else:
        final_img2 = ImageOps.fit(img2, (112,112))

    c2.image(final_img2, caption="Input 2 (Aligned)", width=150)
    emb2, n2 = extract_style_adaface(model, final_img2)

    # --- K·∫øt qu·∫£ ---
    if emb1 is not None and emb2 is not None:
        score = compute_cosine(emb1, emb2)
        
        st.markdown(f"<h2 style='text-align: center; color: #4CAF50;'>Sim: {score:.4f}</h2>", unsafe_allow_html=True)
        
        if score >= threshold:
            st.success("‚úÖ SAME PERSON")
        else:
            st.error("‚ùå DIFFERENT PERSON")
            
        st.progress(max(0.0, min(1.0, float(score))))
        
        with st.expander("Debug Info"):
            st.write(f"Norm 1: {n1:.2f} | Norm 2: {n2:.2f}")
            st.caption("N·∫øu Norm th·∫•p (<20) c√≥ th·ªÉ ·∫£nh m·ªù ho·∫∑c model ch∆∞a kh·ªõp.")