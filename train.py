import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
import os
from tqdm import tqdm
from torch.amp import GradScaler, autocast

# Models
from models.model_ir_se50 import IR_SE50
from models.facenet_model import FaceNetBackbone
from models.ada_face import AdaFaceNet

# Utils
from utils import (
    SiameseWrapper,
    ContrastiveLoss,
    CACDPairDataset,
    AdaFaceLoss,
    CACDClassificationDataset_IdentitySplit,
    run_epoch_adaface,
    CACDClassificationDataset_AgeGap
)

# ===================== CONFIG =====================
CONFIG = {
    "data_dir": "data/CACD_Cropped_112",
    "weight_path": "weights/InsightFace_Pytorch%2Bmodel_ir_se50.pth",
    "img_size": 112,
    "embedding_size": 512,
    "batch_size": 24,             # ‚Üì gi·∫£m batch
    "epochs": 50,
    "lr_siamese": 1e-4,
    "lr_adaface": 5e-5,           # ‚Üì gi·∫£m LR m·∫°nh
    "margin": 1.0,
    "pairs_per_epoch": 8000,
    "val_pairs": 2000
}

# # ================= LOAD WEIGHTS =================
# def load_backbone_weights(model, path):
#     if not os.path.exists(path):
#         print(f"‚ö†Ô∏è Kh√¥ng th·∫•y weights t·∫°i {path}")
#         return

#     print(f"üì• Loading IR-SE50 weights from {path}")
#     state_dict = torch.load(path, map_location='cpu')

#     clean = {k.replace('module.', ''): v for k, v in state_dict.items()}
#     model.load_state_dict(clean, strict=False)
#     print("‚úÖ Weights loaded")


# ================= MODEL =================
def get_model(args, device, num_classes=None):

    if args.model == "adaface":
        model = AdaFaceNet(num_classes, CONFIG['embedding_size'], CONFIG['weight_path'])
        #load_backbone_weights(model.backbone, CONFIG['weight_path'])
        return model.to(device)

    if args.model == "facenet":
        backbone = FaceNetBackbone(CONFIG['embedding_size'])
    elif args.model == "arcface":
        backbone = IR_SE50(CONFIG['embedding_size'])
        #load_backbone_weights(backbone, CONFIG['weight_path'])
    else:
        raise ValueError("Invalid model")

    return SiameseWrapper(backbone).to(device)


# ================= FREEZE - 3 STAGE =================

def freeze_stage(model, stage):
    """
    Stage 1: only head
    Stage 2: head + output_layer
    Stage 3: + last 2 backbone blocks
    """

    if hasattr(model, 'backbone'):
        backbone = model.backbone
    else:
        backbone = model

    # Freeze ALL
    for p in model.parameters():
        p.requires_grad = False

    # Stage 1 ‚Äì ONLY HEAD
    for p in model.head.parameters():
        p.requires_grad = True

    if stage >= 2:
        if hasattr(backbone, 'output_layer'):
            for p in backbone.output_layer.parameters():
                p.requires_grad = True

    if stage >= 3 and hasattr(backbone, 'body'):
        total_blocks = len(backbone.body)
        for i in range(total_blocks - 2, total_blocks):
            for p in backbone.body[i].parameters():
                p.requires_grad = True

    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ Stage {stage}: Trainable {trainable:,}/{total:,} ({100*trainable/total:.2f}%)")


# ================= TRAIN LOOP ‚Äì SIAMESE =================
def run_epoch_siamese(model, loader, criterion, optimizer, device, is_train=True):

    model.train() if is_train else model.eval()
    total_loss = 0
    scaler = GradScaler("cuda", enabled=True)

    with torch.set_grad_enabled(is_train):
        for img1, img2, label in tqdm(loader, leave=False):

            img1, img2, label = img1.to(device), img2.to(device), label.to(device)

            if is_train:
                optimizer.zero_grad()

            with autocast("cuda"):
                out1, out2 = model(img1, img2)
                loss = criterion(out1, out2, label)

            if is_train:
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)
                scaler.step(optimizer)
                scaler.update()

            total_loss += loss.item()

    return total_loss / len(loader)


# ================= MAIN =================
# ... (Ph·∫ßn import v√† freeze_stage gi·ªØ nguy√™n) ...

# ƒê·∫£m b·∫£o ƒë√£ import ƒë·ªß c√°c Dataset t·ª´ utils
from utils import (
    SiameseWrapper, ContrastiveLoss, CACDPairDataset, 
    AdaFaceLoss, CACDClassificationDataset_IdentitySplit, 
    CACDClassificationDataset_AgeGap, # <--- C·∫ßn th√™m c√°i n√†y
    run_epoch_adaface, run_epoch_siamese
)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, required=True, choices=['facenet', 'arcface', 'adaface'])
    parser.add_argument('--subset', type=float, default=0.5)
    parser.add_argument('--split', type=float, default=0.2)
    parser.add_argument('--resume', type=str, default=None)
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"‚öôÔ∏è {device} | Model: {args.model} | Subset: {args.subset}")

    # Transforms
    tf_train = transforms.Compose([
        transforms.Resize((112, 112)),
        # Strong Augmentation gi√∫p ch·ªëng Overfitting
        transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.5),
        transforms.RandomGrayscale(p=0.1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    tf_val = transforms.Compose([
        transforms.Resize((112, 112)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])

    save_dir = "checkpoints"
    os.makedirs(save_dir, exist_ok=True)
    best_val_loss = float('inf')

    # --- KH·ªûI T·∫†O DATASET BAN ƒê·∫¶U (STAGE 1 & 2: Identity Split) ---
    print("\nüì¶ Loading Initial Dataset (Identity Split)...")
    
    if args.model == 'adaface':
        # Dataset Ph√¢n lo·∫°i (Identity Split)
        ds_train = CACDClassificationDataset_IdentitySplit(
            root_dir=CONFIG["data_dir"],
            transform=tf_train,
            subset_ratio=args.subset,
            val_split=args.split,        
            mode="train",
            min_images_per_id=5          
        )

        ds_val = CACDClassificationDataset_IdentitySplit(
            root_dir=CONFIG["data_dir"],
            transform=tf_val,
            subset_ratio=args.subset,
            val_split=args.split,        
            mode="val",
            min_images_per_id=5          
        )
        
        num_classes = len(ds_train.classes)
        print(f"üéØ Classes: {num_classes}")
        
        model = get_model(args, device, num_classes)
        criterion = AdaFaceLoss(label_smoothing=0.1).to(device) # Th√™m label smoothing
        runner = run_epoch_adaface
        
    else:
        # Siamese setup
        ds_train = CACDPairDataset(CONFIG['data_dir'], tf_train, args.subset, args.split, 'train')
        ds_val = CACDPairDataset(CONFIG['data_dir'], tf_val, args.subset, args.split, 'val')
        model = get_model(args, device)
        criterion = ContrastiveLoss().to(device)
        runner = run_epoch_siamese

    # DataLoader
    train_loader = DataLoader(ds_train, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(ds_val, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)

    # Resume
    if args.resume and os.path.exists(args.resume):
        try:
            model.load_state_dict(torch.load(args.resume, map_location=device), strict=False)
            print("‚úÖ Resumed checkpoint.")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói load checkpoint: {e}")

    # --- BI·∫æN ƒêI·ªÄU KHI·ªÇN ---
    optimizer = None
    scheduler = None
    dataset_switched = False 

    print("\nüöÄ START TRAINING WITH PROGRESSIVE UNFREEZING...")
    
    for epoch in range(1, CONFIG['epochs'] + 1):
        
        # ==========================================
        # 1. QU·∫¢N L√ù DATASET & STAGE
        # ==========================================
        current_stage = 0
        lr = 0.001
        
        # Giai ƒëo·∫°n 1 (Ep 1-5): Warm-up Head
        if epoch <= 5:
            current_stage = 1
            lr = 0.0001 
            
        # Giai ƒëo·∫°n 2 (Ep 6-15): M·ªü Output Layer
        elif 6 <= epoch <= 15:
            current_stage = 2
            lr = 0.00002
            
        # Giai ƒëo·∫°n 3 (Ep 16+): Full Fine-tune | Dataset Kh√≥
        else:
            current_stage = 3
            lr = 0.00005 
            
            # --- CHUY·ªÇN ƒê·ªîI DATASET ---
            if args.model == 'adaface' and not dataset_switched:
                print("\nüîÑ SWITCHING TO HARD DATASET (Age-Gap Mining)...")
                # Thay min_gap=3 b·∫±ng 10 ƒë·ªÉ tƒÉng ƒë·ªô kh√≥ th·ª±c s·ª±
                ds_train = CACDClassificationDataset_AgeGap(
                    CONFIG['data_dir'], tf_train, args.subset, args.split, 'train', 
                    min_gap=5, age_mode="both"
                )
                train_loader = DataLoader(ds_train, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)
                dataset_switched = True
                print(f"‚úÖ Dataset switched! New size: {len(ds_train)}")

        # ==========================================
        # 2. √ÅP D·ª§NG FREEZE & OPTIMIZER
        # ==========================================
        if epoch in [1, 6, 16]: 
            print(f"\n--- Epoch {epoch}: Configuring Stage {current_stage} (LR={lr}) ---")
            
            freeze_stage(model, current_stage)
            
            params_to_update = filter(lambda p: p.requires_grad, model.parameters())
            
            if args.model == 'adaface':
                wd = 1e-3 if current_stage < 3 else 5e-4 
                optimizer = optim.SGD(params_to_update, lr=lr, momentum=0.9, weight_decay=wd)
            else:
                optimizer = optim.Adam(params_to_update, lr=lr)
                
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(CONFIG['epochs'] - epoch + 1))

        # ==========================================
        # 3. TRAINING LOOP
        # ==========================================
        if args.model == 'adaface':
            t_loss, t_acc = runner(model, train_loader, criterion, optimizer, device, True)
            print(f"Ep {epoch} | Train Loss: {t_loss:.4f} | Acc: {t_acc:.2f}%")
        else:
            t_loss = runner(model, train_loader, criterion, optimizer, device, True)
            print(f"Ep {epoch} | Train Loss: {t_loss:.4f}")

        # ==========================================
        # 4. VALIDATION
        # ==========================================
        if args.model == 'adaface':
            v_loss, v_acc = runner(model, val_loader, criterion, optimizer, device, False)
            print(f"   >> Val Loss: {v_loss:.4f} | Val Acc: {v_acc:.2f}%")
        else:
            v_loss = runner(model, val_loader, criterion, optimizer, device, False)
            print(f"   >> Val Loss: {v_loss:.4f}")

        # ==========================================
        # 5. SAVE MODEL
        # ==========================================
        if v_loss < best_val_loss:
            best_val_loss = v_loss
            torch.save(model.state_dict(), os.path.join(save_dir, f"{args.model}_best.pth"))
            print("   üî• Best Model Saved")
        
        if scheduler:
            scheduler.step()

        if epoch % 5 == 0:
            torch.save(model.state_dict(), os.path.join(save_dir, f"{args.model}_ep{epoch}.pth"))

    print("\n‚úÖ TRAINING COMPLETED.")

def main1():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='adaface')
    parser.add_argument('--subset', type=float, default=0.5) 
    parser.add_argument('--resume', type=str, required=True, help="Path to Epoch 7 checkpoint")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ STARTING STAGE 3 (HARD MINING) DIRECTLY | Resume: {args.resume}")

    # Config cho Stage 3
    LR_STAGE_3 = 1e-5  # R·∫•t nh·ªè, an to√†n
    BATCH_SIZE = 64    # Gi·ªØ nguy√™n nh∆∞ c≈©
    EPOCHS = 20        # Train th√™m 20 epoch n·ªØa l√† ƒë·ªß

    # Transforms
    tf_train = transforms.Compose([
        transforms.Resize((112, 112)),
        transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.5),
        transforms.RandomGrayscale(p=0.1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    tf_val = transforms.Compose([
        transforms.Resize((112, 112)), transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])

    # --- 1. DATASET: D√ôNG HARD MINING NGAY L·∫¨P T·ª®C ---
    print("\nüì¶ Loading Hard Mining Dataset (Age Gap > 10)...")
    
    # Train tr√™n t·∫≠p kh√≥
    ds_train = CACDClassificationDataset_IdentitySplit(
        CONFIG['data_dir'], tf_train, args.subset, 0.2, 'train', 
        min_images_per_id=5 # L·ªçc k·ªπ h∆°n ch√∫t
    )
    
    # Val v·∫´n tr√™n t·∫≠p Identity Split (ƒë·ªÉ so s√°nh chu·∫©n)
    ds_val = CACDClassificationDataset_IdentitySplit(
        CONFIG['data_dir'], tf_val, args.subset, 0.2, 'val', min_images_per_id=5
    )
    
    train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

    # --- 2. MODEL & RESUME ---
    # L∆∞u √Ω: Hard Mining dataset s·∫Ω l·ªçc b·ªõt class, n√™n s·ªë class c√≥ th·ªÉ √≠t h∆°n Identity Split
    # Nh∆∞ng ta ph·∫£i init model v·ªõi s·ªë class C≈® (c·ªßa file checkpoint) ƒë·ªÉ load ƒë∆∞·ª£c weight
    # M·∫πo: Init ƒë·∫°i 2000 class (ho·∫∑c s·ªë class l√∫c train Identity), load weight, 
    # ph·∫ßn th·ª´a ·ªü Head s·∫Ω kh√¥ng ƒë∆∞·ª£c update (kh√¥ng sao c·∫£).
    num_classes_dummy = 800 
    
    model = get_model(args, device, num_classes_dummy)
    
    if os.path.exists(args.resume):
        print(f"üì• Loading weights from {args.resume}...")
        state_dict = torch.load(args.resume, map_location=device)
        model.load_state_dict(state_dict, strict=False)
    else:
        raise ValueError("Ph·∫£i cung c·∫•p file checkpoint t·ªët (Epoch 7) ƒë·ªÉ ch·∫°y Stage 3!")

    # --- 3. FREEZE & OPTIMIZER ---
    print("\n‚ùÑÔ∏è Configuring Stage 3 (Unfreeze Last Blocks)...")
    freeze_stage(model, stage=3) # M·ªü kh√≥a Layer 4, Output, Head
    
    # L·∫•y params c·∫ßn train
    params_to_update = filter(lambda p: p.requires_grad, model.parameters())
    
    # Optimizer LR nh·ªè
    optimizer = optim.SGD(params_to_update, lr=LR_STAGE_3, momentum=0.9, weight_decay=1e-3)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    
    criterion = AdaFaceLoss(label_smoothing=0.1).to(device)
    
    # --- 4. TRAINING LOOP ---
    best_val_loss = float('inf')
    
    # C·∫ßn ƒë·ªãnh nghƒ©a l·∫°i runner ƒë·ªÉ ch√®n freeze_bn v√†o
    def runner_stage3(model, loader, criterion, optimizer, device, is_train):
        model.train() if is_train else model.eval()
        # QUAN TR·ªåNG: Freeze BN ngay c·∫£ khi train
        if is_train:
            freeze_bn(model)
            
        total_loss = 0.0
        correct = 0; total = 0
        scaler = GradScaler('cuda', enabled=True)
        
        desc = "HardTrain" if is_train else "Val"
        with torch.set_grad_enabled(is_train):
            pbar = tqdm(loader, desc=desc, leave=False)
            for img, label in pbar:
                img, label = img.to(device), label.to(device)
                
                # Check label range (ph√≤ng h·ªù label c·ªßa t·∫≠p m·ªõi v∆∞·ª£t qu√° num_classes_dummy)
                # N·∫øu label >= num_classes_dummy, b·ªè qua (ƒë·ªÉ tr√°nh l·ªói)
                if label.max() >= num_classes_dummy: continue

                if is_train: optimizer.zero_grad()
                
                with autocast('cuda', enabled=False): # Safe float32
                    cosine, norms = model(img, label)
                    loss, logits = criterion(cosine, norms, label)
                
                if is_train:
                    if torch.isnan(loss): continue
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)
                    scaler.step(optimizer)
                    scaler.update()
                
                _, preds = torch.max(logits, 1)
                correct += (preds == label).sum().item()
                total += label.size(0)
                total_loss += loss.item()
                pbar.set_postfix({'loss': f"{loss.item():.4f}"})
                
        return total_loss / len(loader), (correct / total * 100) if total else 0

    print("\nüî• STARTING HARD MINING FINE-TUNING...")
    for epoch in range(1, EPOCHS + 1):
        t_loss, t_acc = runner_stage3(model, train_loader, criterion, optimizer, device, True)
        print(f"Ep {epoch} | Hard Loss: {t_loss:.4f} | Hard Acc: {t_acc:.2f}% | LR: {optimizer.param_groups[0]['lr']:.2e}")
        
        v_loss, v_acc = runner_stage3(model, val_loader, criterion, optimizer, device, False)
        print(f"   >> Val Loss: {v_loss:.4f} | Val Acc: {v_acc:.2f}%")
        
        if v_loss < best_val_loss:
            best_val_loss = v_loss
            torch.save(model.state_dict(), os.path.join("checkpoints", f"adaface_stage3_best.pth"))
            print("   üî• Best Stage 3 Model Saved")
            
        scheduler.step()

def freeze_bn(model):
    """
    ƒê√≥ng bƒÉng c√°c l·ªõp Batch Normalization.
    Chuy·ªÉn ch√∫ng sang ch·∫ø ƒë·ªô eval() ƒë·ªÉ kh√¥ng update running mean/var.
    """
    for module in model.modules():
        if isinstance(module, nn.modules.batchnorm._BatchNorm):
            module.eval()

# if __name__ == '__main__':
#     main()

if __name__ == '__main__':
    main1()