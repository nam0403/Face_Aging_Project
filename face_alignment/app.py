import streamlit as st
from PIL import Image, ImageOps
import numpy as np
import torch
import torch.nn.functional as F
import os
import align

# =========================================================
# C·∫§U H√åNH
# =========================================================
import os
import torch
import requests
import sys

def download_from_gdrive(id, destination):
    """
    T·∫£i file t·ª´ Google Drive (h·ªó tr·ª£ file k√≠ch th∆∞·ªõc l·ªõn)
    """
    URL = "https://docs.google.com/uc?export=download"

    session = requests.Session()

    response = session.get(URL, params={'id': id}, stream=True)
    token = _get_confirm_token(response)

    if token:
        params = {'id': id, 'confirm': token}
        response = session.get(URL, params=params, stream=True)

    _save_response_content(response, destination)

def _get_confirm_token(response):
    for key, value in response.cookies.items():
        if key.startswith('download_warning'):
            return value
    return None

def _save_response_content(response, destination):
    CHUNK_SIZE = 32768
    
    # L·∫•y t·ªïng k√≠ch th∆∞·ªõc file (n·∫øu c√≥) ƒë·ªÉ hi·ªÉn th·ªã progress (ƒë∆°n gi·∫£n)
    total_length = response.headers.get('content-length')
    
    print(f"‚¨áÔ∏è Downloading to {destination}...")
    
    with open(destination, "wb") as f:
        downloaded = 0
        for chunk in response.iter_content(CHUNK_SIZE):
            if chunk: 
                f.write(chunk)
                downloaded += len(chunk)
                # Hi·ªÉn th·ªã d·∫•u ch·∫•m ƒë·ªÉ b√°o hi·ªáu ƒëang t·∫£i
                if total_length:
                    # Logic hi·ªÉn th·ªã % c√≥ th·ªÉ th√™m ·ªü ƒë√¢y
                    pass
    print("\n‚úÖ Download complete!")

# ==========================================
# CONFIGURATION
# ==========================================

# 1. Thay th·∫ø ID n√†y b·∫±ng ID file th·ª±c t·∫ø tr√™n Google Drive c·ªßa b·∫°n
# V√≠ d·ª• link: drive.google.com/file/d/1A2B3C.../view -> ID l√† 1A2B3C...
GDRIVE_FILE_ID = '1FH81gkKbsLG1EOVn1WjoyfQlJVCpm8p3' 

# 2. T√™n file model s·∫Ω l∆∞u tr√™n m√°y
MODEL_FILENAME = "ir_se_101_temporal_best.pth"

# 3. Ki·ªÉm tra v√† t·∫£i file
if not os.path.exists(MODEL_FILENAME):
    print(f"‚ö†Ô∏è Model file '{MODEL_FILENAME}' not found locally.")
    
    if GDRIVE_FILE_ID == 'YOUR_GDRIVE_FILE_ID_HERE':
        print("‚ùå Error: Please update 'GDRIVE_FILE_ID' in model_loader.py with your real Google Drive File ID.")
        sys.exit(1)
    try:
        download_from_gdrive(GDRIVE_FILE_ID, MODEL_FILENAME)
    except Exception as e:
        print(f"‚ùå Failed to download model: {e}")
        sys.exit(1)
else:
    print(f"‚úÖ Found model file: {MODEL_FILENAME}")

# Trong app.py
from model_loader import MODEL_CHECKPOINT_PATH, DEVICE

print(f"üöÄ Device set to: {DEVICE}")

# Import net.py
try:
    import net
except ImportError:
    st.error("‚ö†Ô∏è L·ªñI: Kh√¥ng t√¨m th·∫•y file `net.py`. H√£y ƒë·∫∑t c√πng th∆∞ m·ª•c.")
    st.stop()

st.set_page_config(page_title="AdaFace Demo", layout="centered")

# ------------------------
# 1. Load Model
# ------------------------
@st.cache_resource
def load_system_model():
    model = net.build_model("ir_101").to(DEVICE)

    device = torch.device(DEVICE)
    
    if os.path.exists(MODEL_CHECKPOINT_PATH):
        print(f"üì• Loading model from {MODEL_CHECKPOINT_PATH}")
        checkpoint = torch.load(MODEL_CHECKPOINT_PATH, map_location=device, weights_only=False)
        
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        elif 'state_dict' in checkpoint:
            # Handle pretrained format
            new_state = {
                k[6:]: v for k, v in checkpoint['state_dict'].items() 
                if k.startswith('model.')
            }
            model.load_state_dict(new_state, strict=False)
        else:
            model.load_state_dict(checkpoint)
    else:
        print(f"‚ö†Ô∏è  Model path not found: {MODEL_CHECKPOINT_PATH}")
        return None
    
    model.to(device)
    model.eval()
    return model

def extract_style_adaface(model, pil_img):
    """
    H√†m n√†y m√¥ ph·ªèng l·∫°i logic:
    np_img -> BGR convert -> Normalize th·ªß c√¥ng -> Tensor -> Model -> Normalize Feature
    """
    try:
        if pil_img.mode != "RGB":
            pil_img = pil_img.convert("RGB")
        np_img = np.array(pil_img) 
        
        bgr_img = ((np_img[:, :, ::-1] / 255.) - 0.5) / 0.5
        tensor = torch.tensor(
            bgr_img.transpose(2, 0, 1)
        ).float().unsqueeze(0).to(DEVICE)

        # 5. Forward Pass
        with torch.no_grad():
            out = model(tensor)
            
            if isinstance(out, (tuple, list)):
                feature = out[0]
            else:
                feature = out
            
            norm_val = torch.norm(feature, p=2, dim=1).item()
            feature = F.normalize(feature, dim=1)

            return feature.cpu().numpy()[0], norm_val

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω ·∫£nh: {e}")
        return None, 0.0

def compute_cosine(a, b):
    return float(np.dot(a, b))

st.title("üîç AdaFace Verification")
model = load_system_model()

if not model:
    st.stop()

with st.sidebar:
    use_mtcnn = st.checkbox("D√πng MTCNN Crop", value=True)
    threshold = st.slider("Ng∆∞·ª°ng (Threshold)", 0.0, 1.0, 0.30, 0.01)

col1, col2 = st.columns(2)
f1 = col1.file_uploader("·∫¢nh 1", type=["jpg", "png", "jpeg"])
f2 = col2.file_uploader("·∫¢nh 2", type=["jpg", "png", "jpeg"])

if f1 and f2:
    st.write("---")
    c1, c2 = st.columns(2)
    
    # --- X·ª≠ l√Ω ·∫¢nh 1 ---
    img1 = Image.open(f1).convert("RGB")
    # B∆∞·ªõc Alignment (C·∫Øt m·∫∑t)
    if use_mtcnn:
        align1 = align.get_aligned_face(image_path=None, rgb_pil_image=img1)
        final_img1 = align1 if align1 else ImageOps.fit(img1, (112,112))
    else:
        final_img1 = ImageOps.fit(img1, (112,112)) # Center crop c∆° b·∫£n
        
    c1.image(final_img1, caption="Input 1 (Aligned)", width=150)
    # G·ªçi h√†m x·ª≠ l√Ω m·ªõi
    emb1, n1 = extract_style_adaface(model, final_img1)

    # --- X·ª≠ l√Ω ·∫¢nh 2 ---
    img2 = Image.open(f2).convert("RGB")
    if use_mtcnn:
        align2 = align.get_aligned_face(image_path=None, rgb_pil_image=img2)
        final_img2 = align2 if align2 else ImageOps.fit(img2, (112,112))
    else:
        final_img2 = ImageOps.fit(img2, (112,112))

    c2.image(final_img2, caption="Input 2 (Aligned)", width=150)
    emb2, n2 = extract_style_adaface(model, final_img2)

    # --- K·∫øt qu·∫£ ---
    if emb1 is not None and emb2 is not None:
        score = compute_cosine(emb1, emb2)
        
        st.markdown(f"<h2 style='text-align: center; color: #4CAF50;'>Sim: {score:.4f}</h2>", unsafe_allow_html=True)
        
        if score >= threshold:
            st.success("‚úÖ SAME PERSON")
        else:
            st.error("‚ùå DIFFERENT PERSON")
            
        st.progress(max(0.0, min(1.0, float(score))))
        
        with st.expander("Debug Info"):
            st.write(f"Norm 1: {n1:.2f} | Norm 2: {n2:.2f}")
            st.caption("N·∫øu Norm th·∫•p (<20) c√≥ th·ªÉ ·∫£nh m·ªù ho·∫∑c model ch∆∞a kh·ªõp.")